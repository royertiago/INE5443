User manual
===========

Introduction
------------

Most of the tools are written in C++ and must be compiled before used.
Run

    make

in the command line to build all tools.
These programs use some features of the new C++14 standard,
so you must have a recent compiler to compile the programs.
(I have tested with `gcc 4.9.1` and `clang 3.5.0`.)

Only the most important command-line options are given here.
All tools described here
answer correctly to the option `--help`,
so the full functionality can be obtained this way.

Most tools read things from `stdin` and write to `stdout`;
this should be controlled with pipes and I/O redirection.


### Short demonstration

    $ make
    ...
    $ datatools/generate_spiral > spiral.data
    $ ./influence_areas < spiral.data > spiral.svg
    # Might take several seconds
    $ firefox influence.svg


Datasets
--------

All programs of these repository work arond "datasets".
A dataset is a file that have a header, containing metadata,
and the data, in a comma-separated format.

The exact format of the dataset can be found in
[`datasets/format.md`](datasets/format.md).
This information is needed only if you want to manipulate the datasets manually
or want to extend some program.

As a dataset sample, we have the
[Iris flower dataset](datasets/iris_flower.data).


Classification
--------------

    ./classify --dataset <dataset>

This is the main program of this repository.
It will read the dataset specified in the command line
and a set of entries to be classified from the standard input.
Each entry is a comma-separated list of floating-point values;
for instance, for the iris flower dataset,
each entry must be a list of four numbers.

    0.5,2.5,3,7

is a valid entry for that dataset.
The program then will try to classify it according to some algorithm
(which algorithm is used can be controlled via command-line).
The result is printed to standard output.
For instance, the entry above is classified as

    Iris-virginica

by the default algorithm.

There should be one such entry per line.

Command line options:

    --hamming
    --manhattan
    --euclidean
Control which distance type will be used.
Note that `--hamming` and `--manhattan` are the same.

    --neighbors <N>
Chose the `k` in the kNN algorithm.


Visualization
-------------

    datatools/generate_svg

It reads a dataset in stdin and writes an image,
in the `svg` format, to stdout.

It, however, only support bidimensional datasets
with a single category --- that is,
no entry can have two different categories.

All the visualization of this set of tools
is centered in this program.
(This program is an workaround while I learn how to implement
some GUI for these programs.)


Areas of Influence
---------------

    datatools/generate_entry_grid

Given a dataset as input, this tool will generate a list of entries
to be used by the classifier as input.
The crucial point is that the generated list of entries
is uniformly distributed in a grid format,
so classifying this file has the same effect of classifying
each pixel in a picture according to the dataset.

The areas of influence of each category in a dataset
could then be visualized by
1. Generating a grid using the tool above
2. Classifying each point using `./classify`
3. Combining the grid with the output of `./classify`
4. Plotting in SVG.

The program

    ./influence_areas

automate the procedure explained above.
Given a dataset in stdin, it writes a SVG to stdout
with the entries generated by the prevois program
classified and colorized.
Most command-line options of `./classify` and `datatools/generate\_svg`
are passed untouched to these programs.

An important command-line option is `--density <N>`.
If set to a value different than 1, this will cause the grid
to be generated more sparsely, reducing both the accuracy
and the execution time of `./classify`.
By default, the density is `3`.


Generating datasets
-------------------

There are two tools to generate datasets.
The first,

    datatools/subdataset

will produce a subset of the dataset given in standard input.

This subset will have a random selection
of the entries of the original dataset.

An interesting option is `--remaining <file>`:
this will create a file with the data entries
that were not chosen to be part of the subdataset,
but without the categories.

This file can be used to test the classifying algorithm,
since its answers can be compared against the solution.
The program

    ./test_classifier

automates this test. Run `./test_classifier --help` for more information.

The other tool to generate datasets is

    datatools/generate_spiral

It will generate a spiral dataset, with some noise.
Several aspects of the algorithm can be tweaked,
like the number of spirals, the noise intensity
and the number of points.
For more information, run `datatools/generate_spiral --help`.
